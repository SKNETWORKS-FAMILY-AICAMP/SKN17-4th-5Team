import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, StoppingCriteria, StoppingCriteriaList
#from langchain.prompts import PromptTemplate
from langchain_core.prompts import PromptTemplate
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_core.output_parsers import BaseOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from dotenv import load_dotenv
from huggingface_hub import login
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # ë²¡í„°DB ê²½ë¡œ ì„¤ì •ìš©

load_dotenv()
login(token=os.getenv('HF_TOKEN'))
#token = os.getenv("HF_TOKEN")
#model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B"
model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"

"""
pip3 install torch torchvision
pip install torch transformers langchain-core langchain-community chromadb langchain-chroma sentence-transformers langchain-huggingface
"""

# 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch.backends.mps.is_available():
    device_str = "mps"
elif torch.cuda.is_available():
    device_str = "cuda"
else:
    device_str = "cpu"
#print(f"[Device] Using: {device_str}")


# 2. sLLM ë¡œë“œ (HyperCLOVAX)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device_str)

class StopOnTokens(StoppingCriteria):
    def __init__(self, stop_token_ids):
        self.stop_token_ids = stop_token_ids
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_id in self.stop_token_ids:
            if stop_id in input_ids[0]:
                return True
        return False

stop_tokens = [tokenizer.eos_token_id]
stopping_criteria_list = StoppingCriteriaList([StopOnTokens(stop_tokens)])

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=device_str,
    max_new_tokens=512,  
    temperature=0.8,      
    return_full_text=False,
    stopping_criteria=stopping_criteria_list
)

llm = HuggingFacePipeline(pipeline=generator)


# 3. Vector DB ë¶ˆëŸ¬ì˜¤ê¸°
def load_vectordb(persist_dir="vector_store_q_only"):
    db_path = os.path.join(BASE_DIR, persist_dir)
    embedding_model = HuggingFaceEmbeddings(model_name="nlpai-lab/KURE-v1")
    return Chroma(persist_directory=db_path, embedding_function=embedding_model)


def get_retriever(db, mode="similarity", k=1):
    if mode == "similarity":
        return db.as_retriever(search_type="similarity", search_kwargs={"k": k})
    elif mode == "mmr":
        return db.as_retriever(search_type="mmr", search_kwargs={"k": k, "lambda_mult": 0.5})
    elif mode == "threshold":
        return db.as_retriever(search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.7, "k": k})
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ëª¨ë“œ: {mode}")


# í›„ì²˜ë¦¬ íŒŒì„œ (ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°)
""" 
class CleanOutputParser(BaseOutputParser[str]):
    def parse(self, text: str) -> str:
        match = re.search(r"ë‹µë³€\s*[:ï¼š]\s*(.*)", text, re.DOTALL)
        if match:
            text = match.group(1)

        text = re.sub(r"(ìš´ì „ì:|ê³ ë¼ë‹ˆ:|ì •ë‹µ:).*", "", text)
        text = re.sub(r"---.*", "", text)
        text = text.strip()

        # ë¬¸ì¥ 4ì¤„ ì´ìƒì´ë©´ ì•ë¶€ë¶„ë§Œ ë‚¨ê¹€
        lines = text.split("\n")
        if len(lines) > 4:
            text = "\n".join(lines[:4])

        return text.strip() if text else "ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
"""




# 4. í”„ë¡¬í”„íŠ¸
prompt_template = """
ë„ˆëŠ” ì´ˆë³´ ìš´ì „ìë¥¼ ë•ëŠ” ì±—ë´‡ì´ì•¼.  
[ì§ˆë¬¸]ì„ ì½ê³ , ìƒí™© íŒŒì•…ì„ í•œ ë’¤,
[ì°¸ê³ ë¬¸ì„œ]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš´ì „ìì—ê²Œ ê°€ì¥ í•„ìš”í•œ ë‚´ìš©ë§Œì„ ì·¨í•©í•´ ì •í™•í•˜ê²Œ ëŒ€ë‹µí•´.

<ì¶œë ¥ ê·œì¹™> 
- ë°˜ë“œì‹œ ë²ˆí˜¸ ìˆœì„œë¡œ ë‹¨ê³„ë³„ ì œì‹œ.  
- [ì°¸ê³  ë¬¸ì„œì—] "(ì¶œì²˜: ~)"ê°€ ìˆë‹¤ë©´ ê·¸ ë¶€ë¶„ì€ ë§í•˜ì§€ë§ˆ.
- [ì°¸ê³  ë¬¸ì„œ]ì— ì—†ëŠ” ë‚´ìš©ì´ë©´ "ì•ˆì „ìš´ì „ì„ í•˜ì„¸ìš”"ë¼ê³  ë‹µë³€.

[ì§ˆë¬¸]  
{question}

[ì°¸ê³  ë¬¸ì„œ]  
{context}  
---  
ìµœì¢… ë‹µë³€:
"""


PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["question", "context"]
)


def get_rag_answer(query: str, persist_dir="vector_store_q_only", k=3, search_mode="similarity"):
    db = load_vectordb(persist_dir)
    print("ë¬¸ì„œ ê²€ìƒ‰ì¤‘...")
    retriever = get_retriever(db, mode=search_mode, k=k)
    #retrieved_docs = retriever.get_relevant_documents(query) # ì˜›ë‚  ë°©ì‹
    retrieved_docs = retriever.invoke(query)
    print("ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\n")
    
    # ì°¸ê³ í•œ ë¬¸ì„œ ì¶œë ¥
    
    print("ğŸ“„ **ì°¸ê³ í•œ ì‚¬ë¡€ ë¬¸ì„œë“¤:**")
    for i, doc in enumerate(retrieved_docs, 1):
        # ë¬¸ì„œ ë‚´ìš©ì˜ ì²« 100ìë§Œ í‘œì‹œ
        preview = doc.page_content[:100].replace("\n", " ")
        print(f"{i}. {preview}...")
        if hasattr(doc, 'metadata') and doc.metadata:
            print(f"   ì¶œì²˜: {doc.metadata}")
    print("-" * 50)
    

    def format_docs(docs):
        formatted = []
        context = []
        for i, doc in enumerate(docs):
            question = doc.page_content.strip()
            answer = doc.metadata.get("answer", "").strip() if doc.metadata else ""
            formatted.append(f"[ìƒí™©]: {question}, [ë§¤ë‰´ì–¼]: {answer}")
            #print(f"[ìƒí™©]: {question}, [ë§¤ë‰´ì–¼]: {answer}\n")
            context.append(f"[ë§¤ë‰´ì–¼ {i+1}]: {answer}")
        
        return "\n".join(context)
    
    #print(format_docs(retrieved_docs)) # í…ŒìŠ¤íŠ¸

    rag_chain = (
        {"context": RunnableLambda(lambda x: format_docs(retrieved_docs)), "question": RunnablePassthrough()}
        | PROMPT
        | llm
        #| CleanOutputParser()
    )

    print()
    #print("ì‘ë‹µ ìƒì„±ì¤‘...\n")
    output = rag_chain.invoke(query)
    return output


# 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
"""
if __name__ == "__main__":
    while True:
        q = input("ì§ˆë¬¸: ")
        ans = get_rag_answer(q, persist_dir="vector_store_q_only", search_mode="similarity", k=3)
        print("ìµœì¢… ë‹µë³€:")
        print(ans)
        print()
        print()
        print("-" * 100)

"""
